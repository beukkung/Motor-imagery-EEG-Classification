{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Flatten, Dense, Conv1D, MaxPool1D, Dropout,BatchNormalization,SpatialDropout1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data & Split Data to train&validiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load your data\n",
    "x_all = np.load('X_all.npy')\n",
    "y_all = np.load('y_all.npy')\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_all, y_all,test_size=0.20,random_state=69)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1D-CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Flatten, Dense, Conv1D, MaxPool1D, Dropout,BatchNormalization,SpatialDropout1D\n",
    "# Create sequential model \n",
    "cnn_model = tf.keras.models.Sequential()\n",
    "#Zero CNN layer  with 32 filters, conv window 3, relu activation and same padding\n",
    "cnn_model.add(Conv1D(filters=32, kernel_size=(20,), padding='same', activation=tf.keras.layers.LeakyReLU(alpha=0.001), input_shape = (400, 20)))\n",
    "cnn_model.add(BatchNormalization())\n",
    "#First CNN layer  with 32 filters, conv window 3, relu activation and same padding\n",
    "cnn_model.add(Conv1D(filters=32, kernel_size=(20,), padding='same', activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n",
    "cnn_model.add(BatchNormalization())\n",
    "cnn_model.add(SpatialDropout1D(0.5))\n",
    "#Second CNN layer  with 64 filters, conv window 3, relu activation and same padding\n",
    "cnn_model.add(Conv1D(filters=32, kernel_size=(6,), padding='same', activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n",
    "cnn_model.add(BatchNormalization())\n",
    "#Third CNN layer with Max pooling\n",
    "cnn_model.add(MaxPool1D(pool_size=(2,), strides=2, padding='same'))\n",
    "cnn_model.add(Conv1D(filters=32, kernel_size=(3,), padding='same', activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n",
    "cnn_model.add(SpatialDropout1D(0.5))\n",
    "#Flatten the output\n",
    "cnn_model.add(Flatten())\n",
    "#Add a dense layer with 296 neurons\n",
    "cnn_model.add(Dense(units = 296, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n",
    "#Add a dense layer with 148 neurons\n",
    "cnn_model.add(Dense(units = 148, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n",
    "#Add a dense layer with 74 neurons\n",
    "cnn_model.add(Dense(units = 74, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n",
    "\n",
    "#Softmax as last layer with five outputs\n",
    "cnn_model.add(Dense(units = 4, activation='softmax'))\n",
    "\n",
    "loss = tf.keras.losses.categorical_crossentropy\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "\n",
    "cnn_model.compile(optimizer=optimizer, loss = loss, metrics=['accuracy'])\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import os\n",
    "modelPath = os.path.join(os.getcwd(), 'bestModel.h5')\n",
    "checkpoint = ModelCheckpoint( # set model saving checkpoints\n",
    "    modelPath, # set path to save model weights\n",
    "    monitor='val_loss', # set monitor metrics\n",
    "    verbose=1, # set training verbosity\n",
    "    save_best_only=True, # set if want to save only best weights\n",
    "    save_weights_only=False, # set if you want to save only model weights\n",
    "    mode='auto', # set if save min or max in metrics\n",
    "    period=1 # interval between checkpoints\n",
    "    )\n",
    "\n",
    "earlystopping = EarlyStopping(\n",
    "    monitor='val_loss', # set monitor metrics\n",
    "    min_delta=0.001, # set minimum metrics delta\n",
    "    patience=4, # number of epochs to stop training\n",
    "    restore_best_weights=True, # set if use best weights or last weights\n",
    "    )\n",
    "callbacksList = [checkpoint, earlystopping] # build callbacks list\n",
    "\n",
    "cnn_model_history = cnn_model.fit(x_train, y_train, epochs=100, batch_size = 64, validation_data = (x_val, y_val), callbacks=callbacksList)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load testdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load your testdata \n",
    "testdata = np.load('test.npy')\n",
    "\n",
    "#Prediction\n",
    "ypred = cnn_model.predict(testdata)\n",
    "sub=[]\n",
    "for i in ypred:\n",
    "  sub.append(np.argmax(i)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submission on kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "a=pd.read_csv('sample_submission.csv')\n",
    "\n",
    "num=[]\n",
    "for i in range(400):\n",
    "  num.append(i+1)\n",
    "for i in range(400):\n",
    "  num.append('f'+str(i+1))\n",
    "for i in range(400):\n",
    "  sub.append(None)\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'sample_id':num , 'prediction':sub})\n",
    "df.to_csv('submission.csv',index=False)\n",
    "df = df.reset_index(drop=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
